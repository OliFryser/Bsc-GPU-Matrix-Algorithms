\section{Introduction}

In this project we are going to explore ways to take advantage of the computer's hardware to execute code as fast as possible. First, we will measure the runtime of algorithms on the Central Processing Unit (CPU), where every line of code executes sequentially. Then we will compare it to running the algorithms on the Graphics Processing Unit (GPU), capable of carrying out a large amount of executions in parallel. We will measure the performance of both the CPU and GPU, and reflect on advantages and disadvantages to both approaches. 

To do this we will implement three algorithms from linear algebra in increasing degrees of complexity, namely; matrix addition, matrix multiplication, and matrix inversion. A matrix can be thought of as a 2-dimensional array of numbers. First, we will implement these algorithms on the CPU to establish a baseline implementation. Next, we will implement them on the GPU, and hopefully see a performance increase. We will program our algorithms in the C-programming language for fine grained memory control. We will use the C-

Our workflow will be to incrementally improve our algorithm implementations and closely measure the performance of every version. For this purpose we will develop an automated tool for measuring, storing and visualizing our results. This way we can best fine tune every implementation. 

% cuda C for GPU